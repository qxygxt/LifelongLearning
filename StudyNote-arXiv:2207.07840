This is the cite of my learning paper:
Du K, Li L, Lyu F, et al. Class-Incremental Lifelong Learning in Multi-Label Classification[J]. arXiv preprint arXiv:2207.07840, 2022.
Available at https://arxiv.org/abs/2207.07840

一、读论文前要掌握的基本概念
1. 类增量学习：构建了一个统一的可进化分类器，该分类器从顺序图像数据流中在线学习新的类，并对看到的类实现多类分类。
2. 文中提到的类增量学习有两类，都存在灾难性遗忘的问题：
    （1） Lifelong Single-Label (LSL) classification
          已有的一些方法，比如LwF,对灾难性遗忘作出了解决。
          但只考虑single-labelled input data，因此生活中不实用。
    （2）Lifelong Multi-Label (LML) classification
         LML模型可以通过学习新的类来连续识别多个标签。
         partial labels困难：训练数据可能包含过去任务的标签。
    
二、 论文概述
LML classification的关键性挑战在于构建label间的关系以及减少灾难性遗忘。
作者构建了基于auto-updated expert机制的AGCN网络和表示label间关系的ACM矩阵来解决这两个挑战。前一个task训练完成的AGCN网络作为expert产生soft label作为ACM矩阵的一部分，将ACM矩阵结合下一个task的训练资料更新网络的参数。
损失函数中除了classification loss还有distillation loss(基于expert机制) 和 relationship-preserving loss function (基于embeddings)分别被用来减缓 class-level forgetting 和 relationship-level forgetting。

三、Methodology
1. Augmented Correlation Matrix (ACM)
  由于 partial label problem,无法直接用数据构造class relation,因此作者使用了ACM来近似地表示所有机器已经看到的classes之间的relationship,并且capture the intra- and
inter-task label independences
1.1 ACM由四部分组成
(1)Old-Old block: 
   表示intra- and inter-task label relationships between old and old classes.
   也就是上一个任务的整个ACM矩阵。
(2)New-New block:
    表示intra-task label relationships among the new classes.
    其每一项是由两个classes的example数量与其中一个class的example数量之比计算得出，并且由于是online data stream，每步都会更新此block。
(3)Old-New block
    表示label relationships between old and new classes
    由上一个任务保存的network，称expert，产生的soft label以及本任务hard label得出。
(4)New-Old block
    表示label relationships between new and old classes
    基于贝叶斯，在已经看见过class i的情况下，再出现class j的概率，同样由soft label以及hard label得出。
2. Augmented Graph Convolutional Network (AGCN)
  AGCN 是一种 two-layer stacked graph model，输入ACM矩阵和embedding(用Glove embedding初始化)，AGCN输出graph presentation.
  将这个graph presentation与一个CNN的feature extractor做点积得出prediction.
  
四、减轻灾难性遗忘的方法
1. 对于the class-level catastrophic forgetting
    借鉴于distillation-based lifelong learning method。
    当我们训练完Task t的网络之后，我们先将它的parameters fixed，将它作为expert network，产生soft label，并且用这些soft label作为训练资料，参与新Task的学习，再更新network的参数。
    基于expert, distillation loss定义为上一个任务的soft label和本次任务的hard label的交叉熵。
2. 对于the relationship-level forgetting
    利用旧的节点嵌入的部分约束使损失函数最小，限制了AGCN parameter的改变，使已建立的标签关系的遗忘得到了缓解 
3. 总的loss被定义为classification loss，以及以上两点的loss这三者的加权和
   在作者最后的超参设置中，classification loss和distillation loss的权重分别仅有0.07和0.93，但relationship-preserving loss 却是1e5，此时性能获得较大的增益，这意味着缓解relationship的灾难性遗忘对LML分类是非常重要的。
   
五、实验
1. 数据集（2组）
Split-COCO: 40 most frequent conceptsfrom 80 classes of MS-COCO
Split-WIDE: 21 most frequent concepts from 81 classesof NUS-WIDE
2. 评价指标 (4个)
mAP, CF1, OF1, and Forgetting measure
3. 结果
AGCN在这四个指标上的表现均好于现阶段最先进的模型，证明了AGCN 在 large-scale multi-label dataset 是非常有效的。
4. 结果的分析
分析是基于ACM矩阵的，在实验结束之后，将ACM矩阵可视化之后可以发现，相关性高的两个classes之间的依赖关系权重比不相关的类要大，这意味着即使旧的class不可用，也可以很好地构建任务内部和任务之间的关系。
